{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOTIF-BASED EXPLAINER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook represents the analysis of the previously built datasets to explain changes in inclusion levels of Knockdown responsive exons just based on motif ocurrences. \n",
    "Each RBP represents a dataset. Ideally, the motifs that would explain given dPSI values (or exon strength) would highlight the RBPs that were knockdown in the given experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n",
      "join: Strand data from other will be added as strand data to self.\n",
      "If this is undesired use the flag apply_strand_suffix=False.\n",
      "To turn off the warning set apply_strand_suffix to True or False.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "from explainer.datasets.global_explain import GlobalDataset\n",
    "from gtfhandle.utils import file_to_bed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the spliceAI predictions for all the union of the different exons present in all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpliceAI predictions for all exon coordinates\n",
    "SPLICEAI_PREDS_PATH = \"/Users/pbarbosa/Desktop/NFS_Pedro/phd/rbp_data/HepG2/tabular_dataset_generation/FINAL_SPLICEAI_PREDS.tsv.gz\"\n",
    "#SPLICEAI_PREDS_PATH = \"/Users/pbarbosa/git_repos/interpret_splicing/notebooks/1_BUILD_DATASETS/FINAL_SPLICEAI_PREDS.tsv.gz\"\n",
    "SPLICEAI_PREDS = pd.read_csv(SPLICEAI_PREDS_PATH, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "# Final tabular output \n",
    "#FINAL_TABULAR = pd.read_csv(\"/Users/pbarbosa/Desktop/NFS_Pedro/phd/rbp_data/HepG2/tabular_dataset_generation/FINAL_READY_DATASET.tsv.gz\", sep=\"\\t\", low_memory=False)\n",
    "\n",
    "# GTF CACHE\n",
    "GTF_CACHE = \"/Users/pbarbosa/Desktop/NFS_Pedro/phd/rbp_data/HepG2/cache_gtf/\"\n",
    "\n",
    "# FASTA \n",
    "FASTA = \"/Users/pbarbosa/MEOCloud/genome_utilities/hg38/GRCh38.primary_assembly.genome.fa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['ref_donor_cassette', 'ref_acceptor_cassette',\n",
    "                'ref_acceptor_upstream', 'ref_donor_downstream',\n",
    "                'ref_donor_upstream', 'ref_acceptor_downstream']\n",
    "\n",
    "def generateDatasets(df: pd.DataFrame, \n",
    "                     analysis_name: str,\n",
    "                     use_motif_counts: bool,\n",
    "                     use_motif_locations: bool,\n",
    "                     use_motif_distances: bool, \n",
    "                     **kwargs):\n",
    "    \"\"\"\n",
    "    Generates tabular datasets\n",
    "    \"\"\"\n",
    "    #print(\"Processing {}\".format(name))\n",
    "    os.makedirs('tabular_datasets', exist_ok=True)\n",
    "    \n",
    "    map_d = {'occurrences_alone': 'S1', 'occurrences_based_on_location': 'S2', 'occurrences_based_on_location_and_distances': 'S3'}\n",
    "    analysis_name = map_d.get(analysis_name)\n",
    "   \n",
    "    out = []\n",
    "    for exon_group, data in df.groupby('exon_group'):\n",
    "        \n",
    "        rbp = df.name\n",
    "        outdir = os.path.join(os.path.dirname(SPLICEAI_PREDS_PATH), rbp + \"_\" + exon_group)\n",
    "        kwargs['outbasename'] = rbp + \"_\" + exon_group\n",
    "        \n",
    "        dt = GlobalDataset(data,\n",
    "                    outdir=outdir,\n",
    "                    normalize_by_length=True,\n",
    "                    use_motif_counts=use_motif_counts,\n",
    "                    use_submotif_counts=False,\n",
    "                    use_motif_locations=use_motif_locations,\n",
    "                    use_motif_distances=use_motif_distances,\n",
    "                    **kwargs)\n",
    "        \n",
    "        dt.data = dt.data.drop(columns=cols_to_drop + list(dt.data.filter(regex='^len_')))\n",
    "        out.append(dt.data)\n",
    "    \n",
    "    data = pd.concat(out, axis=0)\n",
    "    data.to_csv(os.path.join('tabular_datasets', '{}_{}.tsv.gz'.format(rbp, analysis_name)), compression='gzip', sep=\"\\t\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO GENERATE DATASETS \n",
    "# kwargs = {'subset_RBPs': 'encode_in_rosina2017'}   \n",
    "\n",
    "# data_config = {'occurrences_alone': [True, False, False],\n",
    "#                'occurrences_based_on_location': [False, True, False],\n",
    "#                'occurrences_based_on_location_and_distances': [False, False, True]}\n",
    "\n",
    "# SPLICEAI_PREDS = SPLICEAI_PREDS[SPLICEAI_PREDS.RBP == \"CELF1\"]\n",
    "# all_datasets = {}\n",
    "# for analysis, flags in data_config.items():\n",
    "#     df = SPLICEAI_PREDS.groupby(['RBP']).progress_apply(generateDatasets, \n",
    "#                                                         analysis_name=analysis,\n",
    "#                                                         use_motif_counts=flags[0],\n",
    "#                                                         use_motif_locations=flags[1],\n",
    "#                                                         use_motif_distances=flags[2],\n",
    "#                                                         **kwargs).reset_index(drop=True)\n",
    "    \n",
    "#     all_datasets[analysis] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO LOAD PROCESSED DATASETS\n",
    "all_datasets = {'occurrences_alone': pd.read_csv('tabular_datasets/CELF1_S1.tsv.gz', sep=\"\\t\"),\n",
    "               'occurrences_based_on_location': pd.read_csv('tabular_datasets/CELF1_S2.tsv.gz', sep=\"\\t\")#,\n",
    "               #'occurrences_based_on_location_and_distances': pd.read_csv('tabular_datasets/CELF1_S3.tsv.gz', sep=\"\\t\")\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in all_datasets.items():\n",
    "    all_datasets[k] = v.drop([x for x in cols_to_drop if x in v.columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "def plotFeatureImportances(df: pd.DataFrame, outdir: str, \n",
    "                           basename:str, \n",
    "                           correlated_pairs: dict = None, \n",
    "                           ranks_pval: Union[pd.Series, pd.DataFrame] = None):\n",
    " \n",
    "    def _draw(df, rbp: str, basename:str, ranks_pval, is_correlated_feat: bool = False):\n",
    "            \n",
    "        clrs = ['maroon' if x is True else 'grey' for x in df.is_target]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 9))\n",
    "        g = sns.barplot(x='feat_import', y='feat_names_l' if 'feat_names_l' in df.columns else 'feat_names', orient='h', palette=clrs, data=df.head(40))\n",
    "    \n",
    "        try:\n",
    "            idx = clrs.index('maroon')\n",
    "            for i, p in enumerate(g.patches):\n",
    "\n",
    "                if i == idx:\n",
    "               \n",
    "                    g.annotate('pval: {}'.format(str(ranks_pval[rbp][1]) if is_correlated_feat is False else str(ranks_pval[rbp][3])), \n",
    "                            xy=(p.get_x() + p.get_width() + 0.001, p.get_y()), \n",
    "                            ha = 'left', va = 'top')#,\n",
    "                    #       textcoords = 'offset points')\n",
    "\n",
    "        except ValueError:\n",
    "            a=1\n",
    "            \n",
    "        if is_correlated_feat:\n",
    "            out = '{}/{}_{}_feat_import_using_corrFeat.pdf'.format(outdir, rbp, basename)\n",
    "        else:\n",
    "            out = '{}/{}_{}_feat_import.pdf'.format(outdir, rbp, basename)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out)\n",
    "        plt.close()\n",
    "\n",
    "    rbp = df.name\n",
    "    df = df[df.feat_import > 0].sort_values('feat_import', ascending=False)\n",
    "    \n",
    "    if correlated_pairs is not None:\n",
    "        \n",
    "        corr_feat_at_rbp = correlated_pairs[rbp]\n",
    "        df['feat_names_l'] = df.feat_names.apply(lambda x: x + \" (n_corr={})\".format(str(len(corr_feat_at_rbp[x]))))\n",
    "    \n",
    "    # True RBP-related values    \n",
    "    _target_features = df[df.feat_names.str.contains(rbp)].feat_names  \n",
    "    df['is_target'] = df.feat_names.isin(_target_features) \n",
    "    _draw(df, rbp, basename, ranks_pval)\n",
    "    \n",
    "    # Accounting for highly correlated features\n",
    "    if all(x is not None for x in [correlated_pairs, ranks_pval]):\n",
    "        \n",
    "        corr_feat_at_rbp = correlated_pairs[rbp][rbp] + df[df.feat_names.str.contains(rbp)].feat_names.tolist()\n",
    "\n",
    "        mask = df.feat_names.str.contains('|'.join(corr_feat_at_rbp))\n",
    "        df['is_target'] = mask\n",
    "  \n",
    "        _draw(df, rbp, basename, ranks_pval, is_correlated_feat=True) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def do_simulations(feat_names, target_features, rank_target):\n",
    "    n_permutations = 1000\n",
    "    \n",
    "    smaller_rank = 0\n",
    "    for i in range(n_permutations):\n",
    "    \n",
    "        #shuffling inplace\n",
    "        random.shuffle(feat_names)\n",
    "\n",
    "        _target_features = feat_names[feat_names.isin(target_features)]  \n",
    "        _target_features_rank = _target_features.index[0]\n",
    " \n",
    "        if _target_features_rank < rank_target:\n",
    "            smaller_rank += 1\n",
    "\n",
    "    pval = smaller_rank / n_permutations       \n",
    "    return pval\n",
    "\n",
    "def testRanksOfImportances(df: pd.DataFrame, correlated_pairs: dict = None):\n",
    "    rbp== df.name\n",
    "    ranks = df.sort_values('feat_import', ascending=False).feat_names.reset_index(drop=True)\n",
    "    \n",
    "    target_features = ranks[ranks.str.contains(rbp)]\n",
    "\n",
    "    if target_features.size > 0:\n",
    "        # First rank is kept\n",
    "        target_features_rank = target_features.index[0]\n",
    "        \n",
    "        # Median of ranks for the features belonging to a single RBP\n",
    "        # target_features_rank = np.median(target_features.index)\n",
    "        pval_raw = do_simulations(ranks, target_features.tolist(), target_features_rank)\n",
    "    \n",
    "    else:\n",
    "        pval_raw, target_features_rank = None, None\n",
    "        \n",
    "    if correlated_pairs:\n",
    "        corr_feat_at_rbp = correlated_pairs[rbp][rbp]\n",
    "      \n",
    "        target_features = ranks[ranks.str.contains('|'.join(corr_feat_at_rbp))]\n",
    "        \n",
    "        if target_features.size > 0:\n",
    "            target_features_rank_using_corr_feat = target_features.index[0]\n",
    "            pval_using_match_to_corr_feat = do_simulations(ranks, target_features.tolist(), target_features_rank_using_corr_feat)\n",
    "\n",
    "    else:\n",
    "        pval_using_match_to_corr_feat = None\n",
    "        target_features_rank_using_corr_feat = None\n",
    "    \n",
    "    return target_features_rank, pval_raw, target_features_rank_using_corr_feat, pval_using_match_to_corr_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict exon strength based on motif frequencies at different levels of resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def dummy_regressor(df: pd.DataFrame, value_to_predict: str = \"average_cassette_strength\"):\n",
    "    \"\"\"\n",
    "    Dummy regressor to serve as baseline \n",
    "    to compare with other models\n",
    "    \"\"\"\n",
    "    if df.shape[0] > 100:\n",
    "        df = df.drop(['seq_id', 'target_coordinates', 'exon_group', 'RBP'], axis=1)\n",
    "        \n",
    "        y = df[value_to_predict]\n",
    "        x = df.drop(value_to_predict, axis=1)\n",
    "        \n",
    "        dummy_regr = DummyRegressor(strategy=\"median\")\n",
    "        \n",
    "        cv_results_r2 = cross_validate(dummy_regr, x, y, scoring='r2', cv=10, return_estimator=True)\n",
    "        cv_results_mse = cross_validate(dummy_regr, x, y, scoring='neg_mean_squared_error', cv=10, return_estimator=True)\n",
    "\n",
    "        mses, r2_scores = [],[]\n",
    "        \n",
    "        for i, model in enumerate(cv_results_mse['estimator']):\n",
    "            \n",
    "            #tree.plot_tree(rgr, feature_names=x.columns)\n",
    "            mses.append(cv_results_mse['test_score'][i])\n",
    "            r2_scores.append(cv_results_r2['test_score'][i])\n",
    "        \n",
    "        return round(np.mean([abs(x) for x in mses]), 3), round(np.mean(r2_scores), 3), None, None\n",
    "    \n",
    "    else:\n",
    "        print(\"Not enough data: {}\".format(df.name))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree\n",
    "\n",
    "def dt_regressor(df: pd.DataFrame, \n",
    "                 value_to_predict: str = 'average_cassette_strength'):\n",
    "    \"\"\"\n",
    "    Simple regressor to estimate exon strength \n",
    "    based on motif ocurrences features\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.shape[0] > 100:\n",
    "        df = df.drop(['seq_id', 'target_coordinates', 'exon_group', 'RBP'], axis=1)\n",
    "        \n",
    "        y = df[value_to_predict]\n",
    "        x = df.drop(value_to_predict, axis=1)\n",
    "        \n",
    "        rgr = tree.DecisionTreeRegressor(random_state=0, min_samples_leaf=3, max_depth=5)\n",
    "        \n",
    "\n",
    "        cv_results_r2 = cross_validate(rgr, x, y, scoring='r2', cv=10, return_estimator=True)\n",
    "        cv_results_mse = cross_validate(rgr, x, y, scoring='neg_mean_squared_error', cv=10, return_estimator=True)\n",
    "\n",
    "        mses, r2_scores, feat_import = [],[],[]\n",
    "        \n",
    "        for i, model in enumerate(cv_results_mse['estimator']):\n",
    "            \n",
    "            #tree.plot_tree(rgr, feature_names=x.columns)\n",
    "            feat_import.append(model.feature_importances_)\n",
    "            mses.append(cv_results_mse['test_score'][i])\n",
    "            r2_scores.append(cv_results_r2['test_score'][i])\n",
    "        \n",
    "        return round(np.mean([abs(x) for x in mses]), 3), round(np.mean(r2_scores), 3), np.array(feat_import).mean(axis=0).tolist(), list(x)\n",
    "    \n",
    "    else:\n",
    "        print(\"Not enough data: {}\".format(df.name))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def rf_regressor(df: pd.DataFrame, value_to_predict: str = 'average_cassette_strength'):\n",
    "    \"\"\"\n",
    "    Simple random forest classifier to distinguish exon group \n",
    "    based on motif ocurrences features\n",
    "    \"\"\"\n",
    "    if df.shape[0] > 100:\n",
    "        df = df.drop(['seq_id', 'target_coordinates', 'exon_group', 'RBP'], axis=1)\n",
    "        \n",
    "        y = df[value_to_predict]\n",
    "        x = df.drop(value_to_predict, axis=1)\n",
    "    \n",
    "        rgr = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "        \n",
    "        cv_results_r2 = cross_validate(rgr, x, y, scoring='r2', cv=5, return_estimator=True)\n",
    "        cv_results_mse = cross_validate(rgr, x, y, scoring='neg_mean_squared_error', cv=5, return_estimator=True)\n",
    "        \n",
    "        mses, r2_scores, feat_import = [], [], []\n",
    "        \n",
    "        for i, model in enumerate(cv_results_mse['estimator']):\n",
    "            \n",
    "            #tree.plot_tree(rgr, feature_names=x.columns)\n",
    "            feat_import.append(model.feature_importances_)\n",
    "            mses.append(cv_results_mse['test_score'][i])\n",
    "            r2_scores.append(cv_results_r2['test_score'][i])\n",
    "                 \n",
    "        return round(np.mean([abs(x) for x in mses]), 3), round(np.mean(r2_scores), 3), np.array(feat_import).mean(axis=0).tolist(), list(x)\n",
    "    \n",
    "    else:\n",
    "        print(\"Not enough data: {}\".format(df.name))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def xgb_regressor(df: pd.DataFrame, \n",
    "                  value_to_predict: str = 'average_cassette_strength'):\n",
    "    \"\"\"\n",
    "    Simple xgboost classifier to distinguish exon group \n",
    "    based on motif ocurrences features\n",
    "    \"\"\"\n",
    "    if df.shape[0] > 100:\n",
    "        df = df.drop(['seq_id', 'target_coordinates', 'exon_group', 'RBP'], axis=1)\n",
    "        \n",
    "        y = df[value_to_predict]\n",
    "        x = df.drop(value_to_predict, axis=1)\n",
    "        #y = LabelBinarizer().fit_transform(y).ravel()\n",
    "    \n",
    "        params = {'max_depth': 5, \n",
    "                  'eta': 0.3,\n",
    "                  'eval_metric': 'logloss'}\n",
    "\n",
    "        rgr = xgb.XGBRegressor(use_label_encoder=False, **params)\n",
    "        \n",
    "        cv_results_r2 = cross_validate(rgr, x, y, scoring='r2', cv=5, return_estimator=True)\n",
    "        cv_results_mse = cross_validate(rgr, x, y, scoring='neg_mean_squared_error', cv=5, return_estimator=True)\n",
    "        \n",
    "        mses, r2_scores, feat_import = [], [], []\n",
    "        \n",
    "        for i, model in enumerate(cv_results_mse['estimator']):\n",
    "            \n",
    "            #tree.plot_tree(rgr, feature_names=x.columns)\n",
    "            feat_import.append(model.feature_importances_)\n",
    "            mses.append(cv_results_mse['test_score'][i])\n",
    "            r2_scores.append(cv_results_r2['test_score'][i])\n",
    "                 \n",
    "            \n",
    "        return round(np.mean([abs(x) for x in mses]), 3), round(np.mean(r2_scores), 3), np.array(feat_import).mean(axis=0).tolist(), list(x)\n",
    "    \n",
    "    else:\n",
    "        print(\"Not enough data: {}\".format(df.name))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = { 'Dummy_Regr': dummy_regressor,\n",
    "               'DecisionTree_Regr': dt_regressor, \n",
    "               'RandomForest_Regr': rf_regressor,\n",
    "               'XGBoost_Regr': xgb_regressor}\n",
    "\n",
    "out_rgr = []\n",
    "for dataset_type, df in all_datasets.items():\n",
    "    print(\"Dataset type: {}\".format(dataset_type))\n",
    "    \n",
    "    for rgr_name, rgr in regressors.items():\n",
    "        print(\"Regressor: {}\".format(rgr_name))\n",
    "        output_rgr = df.groupby('RBP').progress_apply(rgr).reset_index().rename({0: 'out'}, axis=1).dropna()\n",
    "\n",
    "        output_rgr[['mse', 'r2_score', 'feat_import', 'feat_names']] = pd.DataFrame(output_rgr.out.tolist(), index=output_rgr.index)\n",
    "        output_rgr.drop('out', axis=1, inplace=True)\n",
    "        output_rgr['regressor'] = rgr_name\n",
    "        output_rgr['dataset'] = dataset_type\n",
    "        output_rgr['RBP'] = output_rgr.RBP.apply(lambda x: x + \" (N={})\".format(str(df[df.RBP == x].shape[0])))\n",
    "        out_rgr.append(output_rgr)\n",
    "        \n",
    "        out_path = os.path.join(dataset_type, rgr_name)\n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "        \n",
    "        if \"Dummy\" not in rgr_name:\n",
    "            feat_import_df = output_rgr.drop(['mse', 'r2_score'], axis=1).explode(['feat_import', 'feat_names'])\n",
    "            feat_import_df.groupby('RBP').apply(plotFeatureImportances, outdir=out_path)     \n",
    "\n",
    "out_rgr = pd.concat(out_rgr)\n",
    "out_rgr.to_csv('regression_exonStrength.tsv.gz', compression='gzip', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "out_rgr['dataset'] = out_rgr.dataset.replace({'occurrences_alone': '_1',\n",
    "                                                'occurrences_based_on_location': '_2',\n",
    "                                                'occurrences_based_on_location_and_distances': '_3'})\n",
    "\n",
    "out_rgr['regressor'] = out_rgr.regressor.str.replace('_Regr', '')\n",
    "out_rgr['rgr'] = out_rgr.regressor + out_rgr.dataset\n",
    "\n",
    "r2score_heat = out_rgr.drop(columns=['feat_import', 'feat_names', 'mse']).pivot(index='RBP', columns='rgr', values='r2_score')\n",
    "g = sns.clustermap(data=r2score_heat, cmap=\"vlag\", figsize=(12,12), yticklabels=True)\n",
    "g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xmajorticklabels(), fontsize=12)\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_ymajorticklabels(), fontsize=7)\n",
    "plt.savefig('regression_exonStrength_r2_scores.pdf')\n",
    "\n",
    "mse_heat = out_rgr.drop(columns=['feat_import', 'feat_names', 'r2_score']).pivot(index='RBP', columns='rgr', values='mse')\n",
    "g = sns.clustermap(data=mse_heat, cmap=\"vlag\", figsize=(12,12), yticklabels=True)\n",
    "g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xmajorticklabels(), fontsize=12)\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_ymajorticklabels(), fontsize=7)\n",
    "plt.savefig('regression_exonStrength_mse_scores.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict dPSI changes based on motif frequencies at different levels of resolution\n",
    "\n",
    "### For that, we need the true dPSI changes observed in the splicing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpsi_data_path = \"/Users/pbarbosa/git_repos/interpret_splicing/notebooks/1_BUILD_DATASETS/7_final_datasets/ALL_CONCAT.tsv.gz\"\n",
    "dpsi_data = pd.read_csv(dpsi_data_path, sep=\"\\t\")\n",
    "\n",
    "kd_data = dpsi_data[['exon_KD', 'Strand_KD', 'gene_name_KD', 'transcript_id_KD', 'transcript_type_KD', 'Existing_exon_KD', 'dPSI_KD', 'RBP']].copy()\n",
    "kd_data.columns = [x.replace(\"_KD\", \"\") for x in kd_data.columns]\n",
    "kd_data['exon_group'] = \"KD\"\n",
    "\n",
    "ctrl_data = dpsi_data[['exon_CTRL', 'Strand_CTRL', 'gene_name_CTRL', 'transcript_id_CTRL', 'transcript_type_CTRL', 'Existing_exon_CTRL', 'dPSI_CTRL', 'RBP']].copy()\n",
    "ctrl_data.columns = [x.replace(\"_CTRL\", \"\") for x in ctrl_data.columns]\n",
    "ctrl_data['exon_group'] = 'CTRL'\n",
    "dpsi_cols = ['target_coordinates', 'RBP', 'exon_group', 'dPSI']\n",
    "dpsi_data = pd.concat([kd_data, ctrl_data]).rename({'exon': 'target_coordinates'}, axis=1)[dpsi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets_with_dpsi = {}\n",
    "for k, v in all_datasets.items():\n",
    "    with_dpsi = pd.merge(v, dpsi_data, on=dpsi_cols[:-1]).drop('average_cassette_strength', axis=1)\n",
    "    all_datasets_with_dpsi[k] = with_dpsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {'Dummy_Regr_dPSI': dummy_regressor,\n",
    "              'DecisionTree_Regr_dPSI': dt_regressor, \n",
    "               'RandomForest_Regr_dPSI': rf_regressor,\n",
    "               'XGBoost_Regr_dPSI': xgb_regressor\n",
    "}\n",
    "out_rgr = []\n",
    "for dataset_type, df in all_datasets_with_dpsi.items():\n",
    "    print(\"Dataset type: {}\".format(dataset_type))\n",
    "    \n",
    "    for rgr_name, rgr in regressors.items():\n",
    "        print(\"Regressor: {}\".format(rgr_name))\n",
    "\n",
    "        output_rgr = df.groupby('RBP').progress_apply(rgr, value_to_predict='dPSI').reset_index().rename({0: 'out'}, axis=1).dropna()\n",
    "\n",
    "        output_rgr[['mse', 'r2_score', 'feat_import', 'feat_names']] = pd.DataFrame(output_rgr.out.tolist(), index=output_rgr.index)\n",
    "        output_rgr.drop('out', axis=1, inplace=True)\n",
    "        output_rgr['regressor'] = rgr_name\n",
    "        output_rgr['dataset'] = dataset_type\n",
    "        output_rgr['RBP'] = output_rgr.RBP.apply(lambda x: x + \" (N={})\".format(str(df[df.RBP == x].shape[0])))\n",
    "        out_rgr.append(output_rgr)\n",
    "        \n",
    "        out_path = os.path.join(dataset_type, rgr_name)\n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "        \n",
    "        if 'Dummy' not in rgr_name:\n",
    "            feat_import_df = output_rgr.drop(['mse', 'r2_score'], axis=1).explode(['feat_import', 'feat_names'])\n",
    "            feat_import_df.groupby('RBP').apply(plotFeatureImportances, outdir=out_path)     \n",
    "\n",
    "out_rgr = pd.concat(out_rgr)\n",
    "out_rgr.to_csv('regression_dPSI.tsv.gz', compression='gzip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "out_rgr['dataset'] = out_rgr.dataset.replace({'occurrences_alone': '_1',\n",
    "                                              'occurrences_based_on_location': '_2',\n",
    "                                              'occurrences_based_on_location_and_distances': '_3'})\n",
    "\n",
    "out_rgr['regressor'] = out_rgr.regressor.str.replace('_Regr', '')\n",
    "out_rgr['rgr'] = out_rgr.regressor + out_rgr.dataset\n",
    "\n",
    "r2score_heat = out_rgr.drop(columns=['feat_import', 'feat_names', 'mse']).pivot(index='RBP', columns='rgr', values='r2_score')\n",
    "g = sns.clustermap(data=r2score_heat, cmap=\"vlag\", figsize=(12,12), yticklabels=True)\n",
    "g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xmajorticklabels(), fontsize=12)\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_ymajorticklabels(), fontsize=7)\n",
    "plt.savefig('regression_dPSI_r2_scores.pdf')\n",
    "\n",
    "\n",
    "mse_heat = out_rgr.drop(columns=['feat_import', 'feat_names', 'r2_score']).pivot(index='RBP', columns='rgr', values='mse')\n",
    "g = sns.clustermap(data=mse_heat, cmap=\"vlag\", figsize=(12,12), yticklabels=True)\n",
    "g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xmajorticklabels(), fontsize=12)\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_ymajorticklabels(), fontsize=7)\n",
    "plt.savefig('regression_dPSI_mse_scores.pdf')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3a7fa92aee0c575de82720ff7c994c61c7be9ffd1c939079d837777cbd42d86"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('ml_genomics': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
